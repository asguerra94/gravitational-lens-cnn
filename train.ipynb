{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "from useful_functions import*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We define the network achitecture for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We highly recommend to use a GPU for train and use this model.\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN architecture\n",
    "scale_channel   = 1\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.input    = nn.Sequential(\n",
    "            # 1                                             #3x128x128\n",
    "            nn.Conv2d(3, 12*scale_channel, kernel_size=3),  #12x128x128\n",
    "            nn.Tanh(),\n",
    "            nn.ConvTranspose2d(12*scale_channel, 12*scale_channel, kernel_size=3),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.hidden_1   = nn.Sequential(\n",
    "            # 2\n",
    "            nn.ConvTranspose2d(12*scale_channel, 12*scale_channel, kernel_size=49), #12x176x176\n",
    "            nn.Tanh(),\n",
    "            nn.Conv2d(12*scale_channel, 12*scale_channel, kernel_size=49),  #12x128x128\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.hidden_2   = nn.Sequential(\n",
    "            # 3\n",
    "            nn.ConvTranspose2d(12*scale_channel, 12*scale_channel, kernel_size=49),\n",
    "            nn.Tanh(),\n",
    "            nn.Conv2d(12*scale_channel, 12*scale_channel, kernel_size=49),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.hidden_3   = nn.Sequential(\n",
    "            # 4\n",
    "            nn.ConvTranspose2d(12*scale_channel, 12*scale_channel, kernel_size=49),\n",
    "            nn.Tanh(),\n",
    "            nn.Conv2d(12*scale_channel, 12*scale_channel, kernel_size=49),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.hidden_4   = nn.Sequential(\n",
    "            # 5\n",
    "            nn.ConvTranspose2d(12*scale_channel, 12*scale_channel, kernel_size=49),\n",
    "            nn.Tanh(),\n",
    "            nn.Conv2d(12*scale_channel, 12*scale_channel, kernel_size=49),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.output     = nn.Sequential(\n",
    "            # 6\n",
    "            nn.Conv2d(12*scale_channel, 1, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1  = self.input(x)\n",
    "        \n",
    "        x2  = self.hidden_1(x1)\n",
    "        x2  = torch.mul(x2, x1)\n",
    "\n",
    "        x3  = self.hidden_2(x2)\n",
    "        x3  = torch.mul(x3, x2)\n",
    "\n",
    "        x4  = self.hidden_3(x3)\n",
    "        x4  = torch.mul(x4, x3)\n",
    "\n",
    "        x5  = self.hidden_4(x4)\n",
    "        x5  = torch.mul(x5, x3)\n",
    "\n",
    "        outputs = self.output(x5)\n",
    "\n",
    "        return outputs\n",
    "    \n",
    "# Prepare our model to implement the given data for train\n",
    "model   = Net()\n",
    "model   = model.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a previous model for a initial state model\n",
    "\n",
    "if device == \"cuda\":\n",
    "    trained_model = torch.load(\"checkpoint.pth\")\n",
    "else:\n",
    "    trained_model = torch.load('checkpoint.pth', map_location=torch.device('cpu'))\n",
    "\n",
    "model.load_state_dict(trained_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data to train our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data is already downloaded, so this line should be avoid. However, to download the respective data just run this part.\n",
    "# Remember decompress folders after download them.\n",
    "\n",
    "# Download data\n",
    "\n",
    "for data_iter in range(1, 9):\n",
    "    url = (\"https://descargas.inf.santiago.usm.cl/train/%s\" + \".tar.gz\") % data_iter\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a proper use of RAM memory, we decided to create a numpy.array with the files directory and, then, load the \n",
    "# numpy.files when is needed\n",
    "\n",
    "file_ep     = np.array([])\n",
    "file_ka     = np.array([])\n",
    "\n",
    "for n in tqdm(np.arange(1, 9)):\n",
    "    EP_label    = glob.glob(f\"{n}/EPSILON/*.npy\")\n",
    "    KA_label    = glob.glob(f\"{n}/KAPPA/*.npy\")\n",
    "\n",
    "    file_ep = np.append(file_ep, EP_label)\n",
    "    file_ka = np.append(file_ka, KA_label)\n",
    "\n",
    "file_ep = file_ep.flatten()\n",
    "file_ka = file_ka.flatten()\n",
    "\n",
    "file_ep = file_ep.reshape(-1, 1)\n",
    "file_ka = file_ka.reshape(-1, 1)\n",
    "\n",
    "files   = np.hstack(\n",
    "    (file_ep, file_ka)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_load(n_samples = int, files = files):\n",
    "    # This function load the data and save it in a numpy.array with a shuffle before to randomly pick a set of n_samples of data\n",
    "    np.random.shuffle(files)\n",
    "    \n",
    "    inputs  = np.array([\n",
    "    np.load(files[n, 0]) for n in range(n_samples)\n",
    "        ])\n",
    "    \n",
    "    outputs = np.array([\n",
    "    np.load(files[n, 1]) for n in range(n_samples)\n",
    "        ])\n",
    "    \n",
    "    return inputs, outputs\n",
    "\n",
    "def create_dataset(n_samples = int):\n",
    "    # For train the model the data must be in torch.tensors. For that, we prepare the data using the function data_load and, then,\n",
    "    # implement the data into torch.tensors\n",
    "    X, Y = data_load(n_samples = n_samples)\n",
    "\n",
    "    X   = torch.Tensor(X)\n",
    "    Y   = torch.Tensor(Y)\n",
    "\n",
    "    X   = X.to(device=device)\n",
    "    Y   = Y.to(device=device)\n",
    "\n",
    "    X   = X.view(n_samples, 3, 128, 128)\n",
    "    Y   = Y.view(n_samples, 1, 128, 128)\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the train process for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters for training\n",
    "\n",
    "# n_samples is the size of the dataset used for a train\n",
    "# n_batch is the size of the data from the dataset for implement the backpropagation algorithm\n",
    "# loss_values is a list where the loss.item() are going to be appended to show the training\n",
    "\n",
    "n_samples   = 2000\n",
    "n_batch     = 20\n",
    "loss_values = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Train for our model\n",
    "\n",
    "def train_model(model, optimizer, n_epochs = 20, loss_val = list):\n",
    "    for epochs in range(1, n_epochs+1):\n",
    "        model.train()\n",
    "\n",
    "        input_train, output_train = create_dataset(n_samples = n_samples)\n",
    "    \n",
    "        train_dataset   = TensorDataset(input_train, output_train)\n",
    "        train_loader    = DataLoader(train_dataset, batch_size = n_batch)\n",
    "\n",
    "        running_loss    = 0.0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            \n",
    "            # Resetear los gradientes\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Calcular la pérdida\n",
    "            loss = metric(outputs, labels)\n",
    "\n",
    "            # Backward pass y optimización\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Acumular la pérdida\n",
    "            running_loss    += loss.item()/n_batch\n",
    "\n",
    "        loss_val.append(running_loss)\n",
    "        print('The loss train function value for the epoch number '+str(epochs)+' is', running_loss)\n",
    "\n",
    "        if len(loss_values) > 3:\n",
    "            if running_loss < loss_values[-2]:\n",
    "                torch.save(model.state_dict(), \"checkpoint.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start the train and, for that, we decided to use differents steps in this process. First, we start a train with 20 epochs using\n",
    "# a learning ratio of 1e-3. Secondly, we change the learning ratio to 1e-4 with the same epochs number. Finally, we increase the\n",
    "# eporch number to 100 with a lower learning ratio of 1e-5\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "train_model(model, optimizer = optimizer, n_epochs = 5, loss_val = loss_values)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "train_model(model, optimizer = optimizer, n_epochs = 20, loss_val = loss_values)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "train_model(model, optimizer = optimizer, n_epochs = 50, loss_val = loss_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss function to see a graph of the learning process\n",
    "\n",
    "plt.plot(loss_values, color=\"red\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.title(\"Loss function for our model\")\n",
    "plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
 "nbformat": 4,
 "nbformat_minor": 2
}
